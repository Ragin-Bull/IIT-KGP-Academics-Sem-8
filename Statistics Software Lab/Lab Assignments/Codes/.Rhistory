dfe <- length(Y) - length(model$coefficients)
sigma2 <- sse / dfe
sigma2_ci <- c(sigma2 / qf(0.975, df1 = length(model$coefficients), df2 = dfe),
sigma2 / qf(0.025, df1 = length(model$coefficients), df2 = dfe))
print(paste("95% confidence interval for ﾏタ2:", format(sigma2_ci, digits = 3)))
# c) Test the significance of the coefficients
print(summary(model))
# d) Find R^2
r_squared <- 1 - (sum(model$residuals^2) / sum((Y - mean(Y))^2))
print(paste("R-squared:", format(r_squared, digits = 3)))
X <- c(1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 10, 10, 10, 10, 15, 15, 15, 15)
Y <- c(1.1, 0.7, 1.8, 0.4, 3.0, 1.4, 4.9, 4.4, 4.5, 7.3, 8.2, 6.2, 12.0, 13.1, 12.6, 13.2, 18.7, 19.7, 17.4, 17.1)
n <- length(X)
# Fit a simple linear regression line of Y on X
model <- lm(Y ~ X)
print(model)
# Check if the scatter diagram exhibits a strong linear relationship
r <- cor(X, Y)
print(paste("Correlation coefficient:", format(r, digits = 3)))
# Test the significance of the coefficients
summary(model)
# Calculate 95% confidence intervals for the parameters
confint(model, level = 0.95)
sigma2 <- summary(model)$sigma^2
confint(sigma2, level = 0.95)
X <- c(20, 20, 30, 30, 30, 40, 40, 50, 50, 60)
Y <- c(16.3, 26.7, 39.2, 63.5, 51.3, 98.4, 65.7, 104.1, 155.6, 217.2)
# a) Draw scatter plots of (X, Y) and (X, sqrt(Y))
par(mfrow = c(1, 2))
plot(X, Y, main = "Scatter plot of (X, Y)", xlab = "X", ylab = "Y")
plot(X, sqrt(Y), main = "Scatter plot of (X, sqrt(Y))", xlab = "X", ylab = "sqrt(Y)")
# b) Fit a simple linear regression line between U = sqrt(Y) and X
U <- sqrt(Y)
model <- lm(U ~ X)
print(model)
# c) Find 95% confidence intervals for the slope, intercept, and ﾏタ2
coef_ci <- confint(model, level = 0.95)
print(coef_ci)
sse <- sum(model$residuals^2)
dfe <- length(Y) - length(model$coefficients)
sigma2 <- sse / dfe
sigma2_ci <- c(sigma2 / qf(0.975, df1 = length(model$coefficients), df2 = dfe),
sigma2 / qf(0.025, df1 = length(model$coefficients), df2 = dfe))
print(paste("95% confidence interval for ﾏタ2:", format(sigma2_ci, digits = 3)))
# d) Test the significance of the slope and the intercept
print(summary(model))
# e) Find a 95% confidence interval for the expected stopping distance when the initial speed is 35
new_x <- 35
new_y <- predict(model, newdata = data.frame(X = new_x), interval = "confidence")
print(new_y)
# f) Find a 95% prediction interval for stopping distance when the initial speed is 35
new_pi <- predict(model, newdata = data.frame(X = new_x), interval = "prediction")
print(new_pi)
# g) Carry out a lack of fit analysis
anova(model)
# Load the data
# Load the data
X <- c(1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 10, 10, 10, 10, 15, 15, 15, 15)
Y <- c(1.1, 0.7, 1.8, 0.4, 3.0, 1.4, 4.9, 4.4, 4.5, 7.3, 8.2, 6.2, 12.0, 13.1, 12.6, 13.2, 18.7, 19.7, 17.4, 17.1)
n <- length(X)
# Fit a simple linear regression line of Y on X
model <- lm(Y ~ X)
print(model)
# Check if the scatter diagram exhibits a strong linear relationship
r <- cor(X, Y)
print(paste("Correlation coefficient:", format(r, digits = 3)))
# Test the significance of the coefficients
sse <- sum(model$residuals^2)
dfe <- n - length(model$coefficients)
sigma2 <- sse / dfe
stderr_slope <- sqrt(sigma2 / sum((X - mean(X))^2))
stderr_intercept <- sqrt(sigma2 * (1/n + mean(X)^2 / sum((X - mean(X))^2)))
slope_t_stat <- model$coefficients[2] / stderr_slope
intercept_t_stat <- model$coefficients[1] / stderr_intercept
slope_p_value <- 2 * (1 - pt(abs(slope_t_stat), df = dfe))
intercept_p_value <- 2 * (1 - pt(abs(intercept_t_stat), df = dfe))
print(paste("Slope t-statistic:", format(slope_t_stat, digits = 3), ", p-value:", format(slope_p_value, digits = 3)))
print(paste("Intercept t-statistic:", format(intercept_t_stat, digits = 3), ", p-value:", format(intercept_p_value, digits = 3)))
# Calculate 95% confidence intervals for the parameters
coef_ci <- confint(model, level = 0.95)
print(coef_ci)
sigma2_ci <- c(sigma2 / qf(0.975, df1 = length(model$coefficients), df2 = dfe),
sigma2 / qf(0.025, df1 = length(model$coefficients), df2 = dfe))
print(paste("95% confidence interval for ﾏタ2:", format(sigma2_ci, digits = 3)))
# Exercise - 1
X <- c(20, 20, 30, 30, 30, 40, 40, 50, 50, 60)
y <- c(16.3, 26.7, 39.2, 63.5, 51.3, 98.4, 65.7, 104.1, 155.6, 217.2)
# a) Scatter plot
plot(X, y, xlab = "X", ylab = "y", main = "Scatter plot")
# b) Fitting a linear regression model
U <- sqrt(y)
meanX <- sum(X) / length(X)
meanU <- sum(U) / length(U)
beta1 <- sum((X - meanX) * (U - meanU)) / sum((X - meanX)^2)
beta0 <- meanU - beta1 * meanX
UPredicted <- beta0 + beta1 * X
cat("Intercept (beta0):", beta0, "\n")
cat("Slope (beta1):", beta1, "\n")
plot(X, U, xlab = "Initial Speed (X)", ylab = "Square Root of Stopping Distance (U)", main = "Linear Regression")
abline(beta0, beta1, col = "red")
# c) Confidence Intervals for beta0, beta1 and sigma squared
n <- length(X)
df <- n - 2
RSS <- sum((U - UPredicted)^2)
sigma2 <- RSS / df
SESlope <- sqrt(sigma2 / sum((X - meanX)^2))
SEIntercept <- sqrt(sigma2 * (1 / n + meanX^2 / sum((X - meanX)^2)))
t <- qt(0.975, df)
CIForSlope <- c(beta1 - t * SESlope, beta1 + t* SESlope)
CIForIntercept <- c(beta0 - t * SEIntercept, beta0 + t * SEIntercept)
chiLower <- qchisq(0.025, df)
chiUpper <- qchisq(0.975, df)
CISigma2 <- c((df * sigma2) / chiUpper, (df * sigma2) / chiLower)
cat("95% Confidence Interval for Slope (beta1):", CIForSlope, "\n")
cat("95% Confidence Interval for Intercept (beta0):", CIForIntercept, "\n")
cat("95% Confidence Interval for Sigma^2:", CISigma2, "\n")
# d) Test of Significance for beta1 and beta0
# For Beta0:
alpha <- 0.05
SSE <- sum((U - UPredicted)^2)
StandardErrorBeta0 <- sqrt((SSE / df) * ((1 / n) + (meanX^2 / sum((X - meanX)^2))))
tObserved <- beta0 / StandardErrorBeta0
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta0 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta0 is not significant.\n")
}
# For Beta1
alpha <- 0.05
df <- n-2
SSE <- sum((U - UPredicted)^2)
StandardErrorBeta1 <- sqrt((SSE / df) / sum((X - meanX)^2))
tObserved <- beta1 / StandardErrorBeta1
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta1 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta1 is not significant.\n")
}
# e) Confidence Interval for mean of y given X
X0 <- 35
UPredictedForX0 <- beta0 + beta1 * X0
SEPred <- sqrt(sigma2 * ( 1 / n + (X0 - meanX)^2 / sum((X - meanX)^2)))
df <- length(X) - 2
t <- qt(0.975, df)
CIForU <- c(UPredictedForX0- t * SEPred, UPredictedForX0 + t * SEPred)
CIForY <- CIForU^2
cat("95% Confidence Interval for the expected stopping distance when the initial speed is 35:", CIForY, "\n")
# f) Prediction Interval for a new observation of y given X
X0 <- 35
UPredictedForX0 <- beta0 + beta1 * X0
SEPred <- sqrt(sigma2 * (1 + 1 / n + (X0 - meanX)^2 / sum((X - meanX)^2)))
df <- length(X) - 2
t <- qt(0.975, df)
PIForU <- c(UPredictedForX0- t * SEPred, UPredictedForX0 + t * SEPred)
PIForY <- PIForU^2
cat("95% Confidence Interval for the expected stopping distance when the initial speed is 35:", PIForY, "\n")
# g) Lack of fit analysis
n <- length(X)
m <- length(unique(X))
SSE <- sum((U - beta0 - beta1*X)^2)
SSPE <- 0
for(i in 1:m){
SSPE <- SSPE + sum((sqrt(y[X==unique(X)[i]]) - mean(sqrt(y[X==unique(X)[i]])))^2)
}
SSLOF <- SSE - SSPE
fStat <- (SSLOF/(m-2))/(SSPE/(n-m))
fCritical <- qf(0.95,m-2,n-m)
if(fStat > fCritical){
cat("The model caught lacking","\n")
}else{
cat("The model doesnt not lack in fitting the data","\n")
}
# Exercise - 2
n <- 20
X <- c(1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10)
y <- c(1.1, 0.7, 1.8, 0.4, 1.4, 3.0, 4.5, 4.9, 4.4, 7.3, 7.3, 8.2, 6.2, 13.1, 12.0, 12.6, 13.2, 13.1, 18.7, 19.7)
plot(X, y, xlab = "X", ylab = "y", main = "Scatter plot")
# Fitting a linear regression model
meanX <- sum(X) / length(X)
meanY <- sum(y) / length(y)
beta1 <- sum((X - meanX) * (y - meanY)) / sum((X - meanX)^2)
beta0 <- meanY - beta1 * meanX
YPredicted <- beta0 + beta1 * X
cat("Intercept (beta0):", beta0, "\n")
cat("Slope (beta1):", beta1, "\n")
plot(X, y, xlab = "Known Concentration (X)", ylab = "Computed Concentration (Y)", main = "Linear Regression")
abline(beta0, beta1, col = "red")
# Lack of fit test
m <- length(unique(X))
SSE <- sum((y - beta0 - beta1*X)^2)
SSPE <- 0
for(i in 1:m){
SSPE <- SSPE + sum(((y[X==unique(X)[i]]) - mean((y[X==unique(X)[i]])))^2)
}
SSLOF <- SSE - SSPE
fStat <- (SSLOF/(m-2))/(SSPE/(n-m))
fCritical <- qf(0.95,m-2,n-m)
if(fStat > fCritical){
cat("The model caught lacking","\n")
}else{
cat("The model doesnt not lack in fitting the data","\n")
}
# Correlation Coefficient
correlation <- (n * sum(X * y) - sum(X) * sum(y)) / sqrt((n * sum(X^2) - sum(X)^2) * (n * sum(y^2) - sum(y)^2))
cat("Correlation Coefficient:", correlation, "\n")
# Test of significance for beta1 and beta0
# For Beta0:
alpha <- 0.05
df <- n-2
SSE <- sum((y - YPredicted)^2)
StandardErrorBeta0 <- sqrt((SSE / df) * ((1 / n) + (meanX^2 / sum((X - meanX)^2))))
tObserved <- beta0 / StandardErrorBeta0
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta0 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta0 is not significant.\n")
}
# For Beta1
alpha <- 0.05
SSE <- sum((y - YPredicted)^2)
StandardErrorBeta1 <- sqrt((SSE / df) / sum((X - meanX)^2))
tObserved <- beta1 / StandardErrorBeta1
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta1 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta1 is not significant.\n")
}
# Confidence Intervals for all the parameters
sigma2 <- SSE / df
SESlope <- sqrt(sigma2 / sum((X - meanX)^2))
SEIntercept <- sqrt(sigma2 * (1 / n + meanX^2 / sum((X - meanX)^2)))
t <- qt(0.975, df)
CIForSlope <- c(beta1 - t * SESlope, beta1 + t* SESlope)
CIForIntercept <- c(beta0 - t * SEIntercept, beta0 + t * SEIntercept)
chiLower <- qchisq(0.025, df)
chiUpper <- qchisq(0.975, df)
CISigma2 <- c((df * sigma2) / chiUpper, (df * sigma2) / chiLower)
cat("95% Confidence Interval for Slope (beta1):", CIForSlope, "\n")
cat("95% Confidence Interval for Intercept (beta0):", CIForIntercept, "\n")
cat("95% Confidence Interval for Sigma^2:", CISigma2, "\n")
# Exercise - 3
X <- c(1.00, 1.05, 1.10, 1.15, 1.21, 1.32, 1.44, 1.53, 1.63, 1.79, 1.91, 2.03, 2.12, 2.26, 2.37, 2.51)
y <- c(3.71, 3.81, 3.86, 3.93, 3.96, 4.20, 4.34, 4.51, 4.73, 5.35, 5.74, 6.14, 6.51, 6.98, 7.44, 7.76)
# a) Fitting a second degree polynomial regression model
XSquared <- X^2
designMatrix <- cbind(rep(1, length(X)), X, XSquared)
productMatrix <- (t(designMatrix)) %*% designMatrix
productY <- (t(designMatrix)) %*% y
coefficients <- solve(productMatrix, productY)
yPredicted <- coefficients[1] + coefficients[2] * X + coefficients[3] * X^2
cat("Coefficients:\n")
cat("beta_0: ", coefficients[1], "\nbeta_1: ", coefficients[2], "\nbeta_2: ", coefficients[3], "\n")
# Plotting of the curve
plot(X, y, pch=16, col="blue", xlab="X", ylab="Y", main="Scatter plot with regression curve")
curve(coefficients[1] * 1 + coefficients[2] * x + coefficients[3] * x^2, add = TRUE, col = "red", lwd = 2)
residuals <- y - (designMatrix %*% coefficients)
df <- length(y) - ncol(designMatrix)
standardErr <- sqrt(diag(solve(productMatrix)) * sum(residuals^2)/df)
alpha <- 0.05
tObserved <- coefficients / standardErr
t <- qt(1 - alpha/2, df)
lowerBound <- coefficients - t * standardErr
upperBound <- coefficients + t * standardErr
cat("\n95% Confidence Intervals:\n")
for (i in 1:length(coefficients)) {
cat("beta_", i-1, ": [", lowerBound[i], ", ", upperBound[i], "]\n")
}
# c) Test for significance of coefficients
# Test for significance of each coefficient (using if-else)
for (i in 1:length(coefficients)) {
if (abs(tObserved[i]) > t) {
cat("beta_", i-1, "is Significant (p < 0.05)\n")
} else {
cat("beta_", i-1, "is Not Significant (p >= 0.05)\n")
}
}
# d) Computation of R squared
SSRes <- sum((y - yPredicted)^2)
meanY <- mean(y)
SSTotal <- sum((y - meanY)^2)
R2 <- 1 - (SSRes / SSTotal)
cat("Coefficient of determination (R^2):", R2, "\n")
# Exercise - 4
X <- c(1, 2, 3, 4, 5, 6)
y <- c(1.60, 4.50, 13.80, 40.20, 125.00, 300.00)
plot(X, y, pch = 16, col = "blue", xlab = "x", ylab = "y", main = "Scatter plot with fitted curve")
n <- length(X)
Y <- log(y)
# We note that beta0 is log a and b is beta1
meanX <- sum(X) / length(X)
meanY <- sum(Y) / length(Y)
beta1 <- sum((X - meanX) * (Y - meanY)) / sum((X - meanX)^2)
beta0 <- meanY - beta1 * meanX
a <- exp(beta0)
yPredicted <- beta0 + beta1 * X
# Lack of Fitness Test
SSLF <- sum((Y - yPredicted)^2)
SSE <- sum((Y - meanY)^2)
df1 <- n - 2
df2 <- n - 3
F_statistic <- (SSLF / df1) / (SSE / df2)
alpha <- 0.05
critical_value <- qf(1 - alpha, df1, df2)
cat("F-statistic:", F_statistic, "\n")
cat("Critical value:", critical_value, "\n")
if (F_statistic > critical_value) {
cat("Reject null hypothesis: Lack of fit is significant.\n")
} else {
cat("Accept the null hypothesis: Lack of fit is not significant.\n")
}
# Plotting
curve(a * exp(beta1 * x), from = min(X), to = max(X), col = "red", add = TRUE)
# Significance Testing for Parameters
# For Beta0:
alpha <- 0.05
SSE <- sum((Y - yPredicted)^2)
StandardErrorBeta0 <- sqrt((SSE / df2) * ((1 / n) + (meanX^2 / sum((X - meanX)^2))))
tObserved <- beta0 / StandardErrorBeta0
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta0 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta0 is not significant.\n")
}
# For Beta1
alpha <- 0.05
df <- n-2
SSE <- sum((Y - yPredicted)^2)
StandardErrorBeta1 <- sqrt((SSE / df) / sum((X - meanX)^2))
tObserved <- beta1 / StandardErrorBeta1
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta1 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta1 is not significant.\n")
}
# Exercise - 5
X <- c(2, 3, 4, 5, 6)
y <- c(144.0, 172.80, 207.40, 248.50, 298.50)
plot(X, y, pch = 16, col = "blue", xlab = "X", ylab = "Y", main = "Scatter plot with fitted curve")
Y <- log(y)
n <- length(X)
# We note that beta0 is log a and b is exp(beta1)
meanX <- sum(X) / length(X)
meanY <- sum(Y) / length(Y)
beta1 <- sum((X - meanX) * (Y - meanY)) / sum((X - meanX)^2)
beta0 <- meanY - beta1 * meanX
a <- exp(beta0)
b <- exp(beta1)
yPredicted <- beta0 + beta1 * X
# Lack of Fitness Test
SSLF <- sum((Y - yPredicted)^2)
SSE <- sum((Y - meanY)^2)
df1 <- n - 2
df2 <- n - 3
F_statistic <- (SSLF / df1) / (SSE / df2)
alpha <- 0.05
critical_value <- qf(1 - alpha, df1, df2)
cat("F-statistic:", F_statistic, "\n")
cat("Critical value:", critical_value, "\n")
if (F_statistic > critical_value) {
cat("Reject null hypothesis: Lack of fit is significant.\n")
} else {
cat("Accept the null hypothesis: Lack of fit is not significant.\n")
}
# Significance Testing for Parameters
# For Beta0:
alpha <- 0.05
SSE <- sum((Y - yPredicted)^2)
StandardErrorBeta0 <- sqrt((SSE / df1) * ((1 / n) + (meanX^2 / sum((X - meanX)^2))))
tObserved <- beta0 / StandardErrorBeta0
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta0 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta0 is not significant.\n")
}
# For Beta1
alpha <- 0.05
SSE <- sum((Y - yPredicted)^2)
StandardErrorBeta1 <- sqrt((SSE / df1) / sum((X - meanX)^2))
tObserved <- beta1 / StandardErrorBeta1
pValue <- 2 * pt(abs(tObserved), n-2, lower.tail = FALSE)
if (pValue < alpha) {
cat("Reject null hypothesis: Beta1 is significant.\n")
} else {
cat("Accept the null hypothesis: Beta1 is not significant.\n")
}
# Plotting
curve(a * exp(b * x), from = min(X), to = max(X), col = "red", add = TRUE)
# Load the data
y <- c(2.6, 2.4, 17.32, 15.6, 16.12, 5.36, 6.19, 10.17, 2.62, 2.98, 6.92, 7.06)
x1 <- c(31, 31, 31.5, 31.5, 31.5, 30.5, 31.5, 30.5, 31, 30.5, 31, 30.5)
x2 <- c(21, 21, 24, 24, 24, 22, 22, 23, 21.5, 21.5, 22.5, 22.5)
# Fit the second-order polynomial model
model <- lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2))
# (i) Print the model summary
print(model)
# (ii) Confidence intervals for the coefficients
library(stats)
confint(model)
# (iii) Test for significance of regression coefficients
anova(model)
# (iv) Coefficient of determination
summary(model)$r.squared
# Load the data
y <- c(2.6, 2.4, 17.32, 15.6, 16.12, 5.36, 6.19, 10.17, 2.62, 2.98, 6.92, 7.06)
x1 <- c(31, 31, 31.5, 31.5, 31.5, 30.5, 31.5, 30.5, 31, 30.5, 31, 30.5)
x2 <- c(21, 21, 24, 24, 24, 22, 22, 23, 21.5, 21.5, 22.5, 22.5)
# Fit the second-order polynomial model
model <- lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2))
# (i) Print the model summary
print(model)
# (ii) Confidence intervals for the coefficients
library(car)
# Load the data
y <- c(2.6, 2.4, 17.32, 15.6, 16.12, 5.36, 6.19, 10.17, 2.62, 2.98, 6.92, 7.06)
x1 <- c(31, 31, 31.5, 31.5, 31.5, 30.5, 31.5, 30.5, 31, 30.5, 31, 30.5)
x2 <- c(21, 21, 24, 24, 24, 22, 22, 23, 21.5, 21.5, 22.5, 22.5)
# Fit the second-order polynomial model
model <- lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2))
# (i) Print the model summary
print(model)
# (ii) Confidence intervals for the coefficients
library(stats)
confint(model)
# (iii) Test for significance of regression coefficients
anova(model)
# (iv) Coefficient of determination
summary(model)$r.squared
# Load the data
X1 <- c(152, 183, 171, 165, 158, 161, 149, 158, 170, 153, 164, 190, 185)
X2 <- c(50, 20, 20, 30, 30, 50, 60, 50, 40, 55, 40, 40, 20)
Y <- c(120, 141, 124, 126, 117, 129, 123, 125, 132, 123, 132, 155, 147)
# Fit the multiple regression model
model <- lm(Y ~ X1 + X2)
# (a) Print the model summary
print(model)
# (b) Test for significance of all parameters
anova(model)
# (c) Set up 95% confidence intervals for regression coefficients
library(stats)
confint(model)
# (d) Calculate the coefficient of determination
summary(model)$r.squared
# Exercise-3
# Load the data
data <- data.frame(
Conversion = c(49.0, 50.2, 50.5, 48.5, 47.5, 44.5, 28.0, 31.5, 34.5, 35.0, 38.0, 38.5, 15.0, 17.0, 20.5, 29.5),
Temp = c(1300, 1300, 1300, 1300, 1300, 1300, 1200, 1200, 1200, 1200, 1200, 1200, 1100, 1100, 1100, 1100),
Ratio = c(7.5, 9.0, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 17.0),
Time = c(0.0120, 0.0120, 0.0115, 0.0130, 0.0135, 0.0120, 0.0400, 0.0380, 0.0320, 0.0260, 0.0340, 0.0410, 0.0840, 0.0980, 0.0920, 0.0860)
)
# Fit the full quadratic model
model <- lm(Conversion ~ Temp + Ratio + Time + I(Temp^2) + I(Ratio^2) + I(Time^2) + I(Temp*Ratio) + I(Temp*Time) + I(Ratio*Time), data = data)
# Print the model summary
print(model)
# Examine the correlation matrix
library(corrplot)
cor_matrix <- cor(data)
corrplot(cor_matrix, method = "circle")
# Detect and resolve multicollinearity
library(car)
data <- data.frame(
Conversion = c(49.0, 50.2, 50.5, 48.5, 47.5, 44.5, 28.0, 31.5, 34.5, 35.0, 38.0, 38.5, 15.0, 17.0, 20.5, 29.5),
Temp = c(1300, 1300, 1300, 1300, 1300, 1300, 1200, 1200, 1200, 1200, 1200, 1200, 1100, 1100, 1100, 1100),
Ratio = c(7.5, 9.0, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 17.0),
Time = c(0.0120, 0.0120, 0.0115, 0.0130, 0.0135, 0.0120, 0.0400, 0.0380, 0.0320, 0.0260, 0.0340, 0.0410, 0.0840, 0.0980, 0.0920, 0.0860)
)
# Fit the full quadratic model
model <- lm(Conversion ~ Temp + Ratio + Time + I(Temp^2) + I(Ratio^2) + I(Time^2) + I(Temp*Ratio) + I(Temp*Time) + I(Ratio*Time), data = data)
# Print the model summary
print(model)
# Examine the correlation matrix
cor_matrix <- cor(data)
print(cor_matrix)
# Detect multicollinearity
diag_vals <- diag(solve(cor_matrix))
print(diag_vals)
# Load the data
data <- data.frame(
Failure = c(1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0),
Temp = c(53, 57, 58, 63, 66, 67, 67, 68, 69, 70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)
)
# Load the data
Failure <- c(1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0)
Temp <- c(53, 57, 58, 63, 66, 67, 67, 68, 69, 70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)
# Fit the logistic regression model
model <- glm(Failure ~ Temp, family = binomial)
data <- data.frame(
Failure = c(1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0),
Temp = c(53, 57, 58, 63, 66, 67, 67, 68, 69, 70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)
)
data <- data.frame(
Failure = c(1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0),
Temp = c(53, 57, 58, 63, 66, 67, 67, 68, 69, 70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)
)
data <- data.frame(
Failure = c(1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),
Temp = c(53, 57, 58, 63, 66, 67, 67, 67, 68, 69, 70, 70, 70, 70, 70, 75, 75, 76, 76, 76, 76, 78, 79, 81)
)
# Fit the logistic regression model
model <- glm(Failure ~ Temp, data = data, family = binomial)
# Print the model summary
print(model)
# Test for significance of the regression coefficient
library(stats)
summary(model)$coefficients
